# ğŸŒ¬ï¸ Proyecto: ComparaciÃ³n de Arquitecturas Spark para Streaming de ContaminaciÃ³n del Aire

**Curso**: Arquitectura de Sistemas Distribuidos - ITAM  
**Semestre**: OtoÃ±o 2025  
**Equipo**: 2 personas  
**Fecha Entrega**: Semana 17

---

## ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto implementa una **aplicaciÃ³n de Spark Structured Streaming** que:

1. **Captura datos en tiempo real** desde la Air Pollution API (cada 5-10 minutos)
2. **Calcula estadÃ­sticos** en ventanas deslizantes (min, max, media, varianza)
3. **Visualiza en tiempo real** mediante un dashboard interactivo
4. **Entrena un modelo de ML** con datos acumulados
5. **Predice niveles de AQI** en nuevo flujo de streaming
6. **Compara desempeÃ±o** entre diferentes arquitecturas:
   - Spark Standalone (local, con/sin GPU)
   - Google Colab
   - AWS

---

## ğŸ¯ Objetivos EspecÃ­ficos

âœ… Desarrollar aplicaciÃ³n en PySpark para capturar datos en tiempo real  
âœ… Calcular indicadores estadÃ­sticos en tiempo real (min, max, media, varianza)  
âœ… Entrenar modelo de clasificaciÃ³n de AQI (niveles 1-5)  
âœ… Reactivar streaming para predicciÃ³n en tiempo real  
âœ… Comparar ejecuciÃ³n usando **6+ mÃ©tricas de Spark UI**  
âœ… Documentar diferencias entre arquitecturas  

---

## ğŸš€ Inicio RÃ¡pido

### 1. ConfiguraciÃ³n Inicial

```bash
# Navegar al proyecto
cd /Users/miguelmonreal/Desktop/Semestres/OtoÃ±o2025/final_proyect_arqui

# Activar virtual environment
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env si necesitas cambiar API key o coordenadas
```

### 2. Ejecutar Dashboard

```bash
# Terminal 1: Iniciar dashboard
cd src
python dashboard.py

# Acceso: http://localhost:8050
```

El dashboard mostrarÃ¡:
- ğŸ“Š **Flujo en tiempo real**: GrÃ¡fico de serie temporal del AQI
- ğŸ“ˆ **Histograma**: DistribuciÃ³n de niveles de AQI
- ğŸ“¦ **Boxplot**: DistribuciÃ³n de concentraciÃ³n de gases
- ğŸ”¢ **MÃ©tricas**: AQI actual, promedios, mÃ¡ximos

### 3. Probar ConexiÃ³n a API

```bash
# Terminal 2: Test de API
cd src
python api_client.py
```

DeberÃ­a mostrar datos JSON con estructura:
```json
{
  "timestamp": "2025-01-15T14:30:00",
  "lat": 19.4326296,
  "lon": -99.3030,
  "aqi": 3,
  "co": 234.56,
  "no2": 45.23,
  "pm25": 18.5,
  ...
}
```

### 4. Entrenar Modelo (DespuÃ©s de recolectar datos)

```bash
# DespuÃ©s de ~1 hora de recolecciÃ³n de datos
cd src
python model_training.py
```

Genera:
- `models/aqi_model_random_forest_*.pkl` - Modelo entrenado
- `models/aqi_model_random_forest_*_scaler.pkl` - Scaler de features
- `models/aqi_model_random_forest_*_metadata.json` - MÃ©tricas y metadata

### 5. Spark Streaming (Opcional - MÃ¡s Avanzado)

```bash
# Terminal 3: Spark Streaming
cd src
spark-submit spark_streaming.py

# Spark UI: http://localhost:4040
```

---

## ğŸ“ Estructura del Proyecto

```
final_proyect_arqui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_client.py              # Cliente para Air Pollution API
â”‚   â”œâ”€â”€ dashboard.py               # Dashboard Dash/Plotly (tiempo real)
â”‚   â”œâ”€â”€ spark_streaming.py         # Pipeline Spark Structured Streaming
â”‚   â”œâ”€â”€ model_training.py          # Entrenamiento de modelos ML
â”‚   â””â”€â”€ utils.py                   # Funciones utilitarias
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pollution_data.json        # Datos acumulados (creado automÃ¡ticamente)
â”‚   â”œâ”€â”€ streaming/                 # Datos de Spark (JSON/CSV)
â”‚   â””â”€â”€ checkpoints/               # Checkpoints de Spark Streaming
â”œâ”€â”€ models/
â”‚   â””â”€â”€ aqi_model_*.pkl           # Modelos entrenados
â”œâ”€â”€ notebooks/                     # Jupyter notebooks (anÃ¡lisis)
â”œâ”€â”€ requirements.txt               # Dependencias Python
â”œâ”€â”€ .env.example                   # Variables de entorno
â”œâ”€â”€ run_project.sh                 # Script de ejecuciÃ³n
â”œâ”€â”€ PROJECT_STRUCTURE.md           # Este documento
â”œâ”€â”€ context.md                     # Contexto del proyecto
â””â”€â”€ README.md                      # Este archivo
```

---

## ğŸ“Š Dashboard - CaracterÃ­sticas

### Visualizaciones Incluidas

1. **GrÃ¡fico de Flujo (Live Data Graph)**
   - Serie temporal del AQI
   - ActualizaciÃ³n cada 5 minutos
   - Interactivo (zoom, hover, descarga)

2. **Histograma de Niveles AQI**
   - DistribuciÃ³n de frecuencia
   - ClasificaciÃ³n: Good, Fair, Moderate, Poor, Very Poor
   - CÃ³digo de colores

3. **Boxplot de Gases**
   - DistribuciÃ³n de 6 contaminantes principales
   - CO, NOâ‚‚, Oâ‚ƒ, SOâ‚‚, PM2.5, PM10
   - Quartiles y outliers

4. **Panel de MÃ©tricas**
   - AQI actual y promedio
   - MÃ¡ximos y promedios por gas
   - Total de registros
   - Ãšltima actualizaciÃ³n

5. **Serie Temporal de Gases**
   - EvoluciÃ³n de cada contaminante
   - VisualizaciÃ³n superpuesta

### Controles

- âœ… **BotÃ³n "Iniciar Stream"**: Comienza actualizaciÃ³n automÃ¡tica
- âŒ **BotÃ³n "Detener Stream"**: Pausa recolecciÃ³n
- ğŸ”„ **Auto-refresh**: Cada 5 minutos (configurable)

---

## ğŸ§® MÃ©tricas de Rendimiento (Spark UI)

El proyecto captura **8+ mÃ©tricas** requeridas:

| MÃ©trica | DescripciÃ³n | UbicaciÃ³n Spark UI |
|---------|------------|-----------------|
| **Job Runtime** | Tiempo de ejecuciÃ³n total | Jobs tab |
| **Shuffle Time** | Tiempo en operaciones shuffle | Tasks tab |
| **I/O Operations** | Cantidad de lecturas/escrituras | Tasks tab |
| **Scheduler Delay** | Espera de recursos | Timeline tab |
| **Executor Run Time** | Tiempo de computaciÃ³n real | Executor tab |
| **GC Time** | RecolecciÃ³n de basura | Executor tab |
| **Spill (Memory)** | Desbordamiento a memoria | Executor tab |
| **Spill (Disk)** | Desbordamiento a disco | Executor tab |

### CÃ³mo Capturar MÃ©tricas

**Localmente (Spark Standalone)**:
```bash
# Durante ejecuciÃ³n, accede a:
http://localhost:4040/jobs
http://localhost:4040/stages
http://localhost:4040/executors
```

**En AWS/Colab**:
```bash
# Establecer tÃºnel SSH
ssh -L 4040:localhost:4040 usuario@instancia

# O descarga los event logs:
# s3://tu-bucket/spark-logs/
```

---

## ğŸ“ˆ Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Air Pollution API                  â”‚
â”‚  (cada 5-10 minutos)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  api_client.py                      â”‚
â”‚  - Obtiene datos                    â”‚
â”‚  - Parsea JSON                      â”‚
â”‚  - Enriquece con timestamp          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
    â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard   â”‚  â”‚ Spark Streaming  â”‚
â”‚  (Dash)      â”‚  â”‚ (Structured)     â”‚
â”‚  - Flujo     â”‚  â”‚ - EstadÃ­sticas   â”‚
â”‚  - Histogr.  â”‚  â”‚ - JSON Storage   â”‚
â”‚  - Boxplot   â”‚  â”‚ - Checkpoints    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚            â”‚ Datos Acumulados  â”‚
       â”‚            â”‚ (JSON)            â”‚
       â”‚            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚          â”‚ model_training.py â”‚
       â”‚          â”‚ - Entrenamiento   â”‚
       â”‚          â”‚ - ValidaciÃ³n      â”‚
       â”‚          â”‚ - Guardado        â”‚
       â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Predicciones    â”‚
                    â”‚ en Streaming    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Modelo de ML

### Arquitectura

- **Algoritmo**: Random Forest (por defecto)
- **Alternativas**: Gradient Boosting, Logistic Regression
- **Target**: AQI Level (1-5)
- **Features**: CO, NO, NOâ‚‚, Oâ‚ƒ, SOâ‚‚, NHâ‚ƒ, PM2.5, PM10
- **Train/Test Split**: 80/20

### MÃ©tricas de EvaluaciÃ³n

```
- Accuracy: PrecisiÃ³n general
- Precision/Recall/F1: Por clase
- Confusion Matrix: Errores de clasificaciÃ³n
- Feature Importance: Importancia de variables
```

### Guardar y Cargar Modelos

```python
# Entrenamiento
model = AQIPredictionModel("random_forest")
model.train("../data/pollution_data.json")
model.save("../models")

# Uso posterior
loaded_model = AQIPredictionModel.load("../models/aqi_model_*.pkl")
prediction, confidence = loaded_model.predict_single({
    "co": 234.5,
    "no2": 45.2,
    "pm25": 18.5,
    # ...
})
```

---

## ğŸ—ï¸ ComparaciÃ³n de Arquitecturas

### Configuraciones a Probar

#### 1. **Spark Standalone (Local)**
```bash
# Sin GPU
spark-submit --master local[*] src/spark_streaming.py

# MÃ©tricas esperadas:
# - Job Runtime: ~100-500ms
# - Spill: Bajo (RAM suficiente)
# - GC Time: Bajo (< 10% del runtime)
```

#### 2. **Google Colab**
```bash
# En notebook Colab
!pip install pyspark
# Ejecutar cÃ³digo de Spark
# Spark UI disponible vÃ­a ngrok

# MÃ©tricas esperadas:
# - Job Runtime: ~50-200ms (GPU)
# - Mejor paralelismo
# - GC Time: Bajo
```

#### 3. **AWS EC2**
```bash
# Crear cluster EMR o usar EC2 + Spark
# Configurar Spark para multi-node
# Usar S3 para almacenamiento

# MÃ©tricas esperadas:
# - Job Runtime: Variable (segÃºn cluster size)
# - Shuffle Time: Depende del network
# - Mayor escalabilidad
```

### Tabla Comparativa

```
MÃ©trica               | Local      | Colab      | AWS
---------------------|------------|------------|----------
Job Runtime (ms)     | 100-500    | 50-200     | 50-300
Executor Count       | 1-4        | 2-8        | 4-32+
Memory/GB            | 4-16       | 12         | 16-256
Shuffle Time (ms)    | 10-50      | 20-100     | 50-200
GC Time/Total (%)    | 5-10%      | 3-8%       | 10-15%
Spill/Total (MB)     | < 100      | < 50       | < 500
```

---

## ğŸ“ Informe Final Requerido

El informe debe incluir:

1. **DescripciÃ³n de Arquitecturas**
   - Configur aciÃ³n de cada plataforma
   - Recursos utilizados (CPU, RAM, Storage)

2. **Estructura de Datos**
   - Schema de datos de entrada
   - Transformaciones aplicadas

3. **EstadÃ­sticos Calculados**
   - FÃ³rmulas utilizadas
   - Ejemplos de resultados

4. **AnÃ¡lisis de Rendimiento**
   - GrÃ¡ficos de mÃ©tricas por arquitectura
   - Screenshots de Spark UI
   - Tablas comparativas

5. **Casos de Fallo**
   - Â¿QuÃ© pasa al caer un esclavo?
   - Â¿QuÃ© pasa sin GPU en AWS?
   - RecuperaciÃ³n y tolerancia a fallos

6. **Conclusiones**
   - Arquitectura mÃ¡s eficiente
   - Trade-offs observados
   - Recomendaciones

---

## ğŸ› Troubleshooting

### Dashboard no se abre
```bash
# Verifica puerto disponible
lsof -i :8050
kill -9 <PID>

# Reinicia
python dashboard.py
```

### Error de API
```bash
# Verifica API key en .env
cat .env | grep OPENWEATHER

# Test conexiÃ³n
python api_client.py

# Verifica coordenadas
# LAT=19.4326296, LON=-99.3030 (CDMX)
```

### No hay datos en dashboard
- Espera 5 minutos (intervalo de actualizaciÃ³n)
- Verifica que "Iniciar Stream" estÃ© activado
- Comprueba `data/pollution_data.json`

### Spark no inicia
```bash
# Instala Java si no lo tienes
java -version

# Instala PySpark
pip install pyspark

# Verifica JAVA_HOME
echo $JAVA_HOME
export JAVA_HOME=$(/usr/libexec/java_home)
```

---

## ğŸ“š Dependencias

- **pyspark** >= 3.5.0 - Distributed computing
- **dash** >= 2.14.0 - Web framework
- **plotly** >= 5.17.0 - Interactive plots
- **pandas** >= 2.1.0 - Data manipulation
- **scikit-learn** >= 1.3.0 - Machine learning
- **requests** >= 2.31.0 - HTTP client
- **python-dotenv** >= 1.0.0 - Environment vars

---

## ğŸ” Variables de Entorno (.env)

```ini
# API Configuration
OPENWEATHER_API_KEY=tu_api_key
DEFAULT_LAT=19.4326296
DEFAULT_LON=-99.3030

# Spark Configuration
SPARK_MASTER=local[*]
SPARK_EXECUTOR_MEMORY=4g
SPARK_DRIVER_MEMORY=2g

# Streaming
STREAMING_UPDATE_INTERVAL=300
STREAMING_OUTPUT_DIR=./data/streaming

# Dashboard
DASHBOARD_PORT=8050
DASHBOARD_DEBUG=true
```

---

## ğŸ“ Contacto / Preguntas

- **DocumentaciÃ³n Spark**: https://spark.apache.org/docs/latest/
- **API OpenWeatherMap**: https://openweathermap.org/api/air-pollution
- **Dash Documentation**: https://dash.plotly.com/

---

## ğŸ“… Timeline

| Semana | Hito |
|--------|------|
| 1-4 | Desarrollo e integraciÃ³n |
| 5-12 | RecolecciÃ³n de datos y entrenamiento |
| 13-15 | ComparaciÃ³n de arquitecturas |
| 16 | RedacciÃ³n de informe |
| 17 | Entrega + DemostraciÃ³n |

---

**Ãšltima actualizaciÃ³n**: 28 de Noviembre de 2025

Â¡Bienvenido al proyecto! ğŸš€
