# Estructura del Proyecto de Spark Streaming para ContaminaciÃ³n del Aire

## ğŸ“ Estructura de Directorios

```
final_proyect_arqui/
â”œâ”€â”€ src/                          # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ api_client.py            # Cliente para Air Pollution API
â”‚   â”œâ”€â”€ spark_streaming.py       # Pipeline de Spark Streaming
â”‚   â”œâ”€â”€ dashboard.py             # Dashboard interactivo (Dash)
â”‚   â”œâ”€â”€ model_training.py        # Entrenamiento de modelos ML
â”‚   â””â”€â”€ utils.py                 # Funciones utilitarias
â”œâ”€â”€ notebooks/                   # Jupyter notebooks para anÃ¡lisis
â”‚   â”œâ”€â”€ eda.ipynb               # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ model_training.ipynb    # Entrenamiento de modelos
â”‚   â””â”€â”€ performance_comparison.ipynb  # ComparaciÃ³n de arquitecturas
â”œâ”€â”€ data/                        # Datos
â”‚   â”œâ”€â”€ streaming/              # Datos de Spark Streaming (JSON)
â”‚   â”œâ”€â”€ checkpoints/            # Checkpoints de Spark
â”‚   â””â”€â”€ pollution_data.json     # Datos acumulados para dashboard
â”œâ”€â”€ models/                      # Modelos entrenados
â”‚   â””â”€â”€ aqi_classifier.pkl      # Modelo serializado
â”œâ”€â”€ config/                      # Configuraciones
â”‚   â”œâ”€â”€ spark_config.ini        # Config de Spark
â”‚   â””â”€â”€ dashboard_config.json   # Config del dashboard
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ .env.example                 # Variables de entorno (ejemplo)
â”œâ”€â”€ run_project.sh              # Script para ejecutar
â”œâ”€â”€ README.md                    # DocumentaciÃ³n
â””â”€â”€ context.md                   # Contexto del proyecto

```

## ğŸš€ CÃ³mo Empezar

### 1. ConfiguraciÃ³n Inicial

```bash
# Clonar y entrar al directorio
cd final_proyect_arqui

# Crear virtual environment (si aÃºn no existe)
python3 -m venv venv
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Copiar archivo de configuraciÃ³n
cp .env.example .env
# Editar .env con tus credenciales/preferencias
```

### 2. Probar la API

```bash
# Terminal 1: Probar conexiÃ³n
./run_project.sh api-test
```

### 3. Ejecutar el Dashboard

```bash
# Terminal 2: Iniciar dashboard
./run_project.sh dashboard
# Acceder a: http://localhost:8050
```

### 4. Spark Streaming (Opcional - MÃ¡s Avanzado)

```bash
# Terminal 3: Spark Streaming
./run_project.sh streaming
# Spark UI: http://localhost:4040
```

## ğŸ“Š Componentes Principales

### `api_client.py`
- Clase `AirPollutionClient`: Conecta con Air Pollution API
- MÃ©todos:
  - `get_current_pollution()`: Obtiene datos actuales
  - `stream_pollution_data()`: Stream continuo a intervalo regular
  - `create_spark_stream_from_api()`: IntegraciÃ³n con Spark

### `dashboard.py`
- Clase `PollutionDashboard`: Dashboard interactivo con Dash/Plotly
- CaracterÃ­sticas:
  - âœ… GrÃ¡fico de serie temporal (Flujo de AQI)
  - âœ… Histograma de niveles de AQI
  - âœ… Boxplot de concentraciÃ³n de gases
  - âœ… MÃ©tricas en tiempo real
  - âœ… Control de inicio/parada del stream
  - âœ… Auto-actualizaciÃ³n cada 5 minutos

### `spark_streaming.py`
- Clase `AirPollutionStreaming`: Pipeline de Spark
- MÃ©todos:
  - `read_from_socket()`: Lee desde socket TCP
  - `parse_pollution_data()`: Parsea JSON a DataFrame estructurado
  - `calculate_statistics()`: EstadÃ­sticas en ventanas deslizantes
  - `classify_aqi_level()`: Clasifica AQI 1-5
  - `create_histogram_data()`: Prepara datos para histograma
  - `create_boxplot_data()`: Prepara datos para boxplot

## ğŸ¯ Flujo de Datos

```
Air Pollution API (cada 5-10 min)
           â†“
   api_client.py (AirPollutionClient)
           â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â†“                 â†“
Dashboard (Dash)   Spark Streaming
   â†“                 â†“
VisualizaciÃ³n      JSON Storage
en tiempo real     (para anÃ¡lisis)
   â†“                 â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â†“
   Modelo ML (ClasificaciÃ³n AQI)
           â†“
   Predicciones
```

## ğŸ“ˆ MÃ©tricas Calculadas

### EstadÃ­sticas en Tiempo Real
- **AQI**: Min, Max, Media, Desv. Est.
- **Gases**: CO, NO2, O3, SO2, NH3, PM2.5, PM10
- **Por cada gas**: Min, Max, Media

### Para Spark UI
1. âœ… Tiempo de ejecuciÃ³n de jobs
2. âœ… Shuffle time entre stages
3. âœ… Operaciones I/O
4. âœ… Scheduler Delay
5. âœ… Executor Run Time
6. âœ… GC Time
7. âœ… Spill (Memory/Disk)
8. âœ… Environment

## ğŸ” Monitoreo

### Dashboard (localhost:8050)
- Acceso visual a todos los datos
- Controles de inicio/parada
- MÃ©tricas actuales

### Spark UI (localhost:4040)
- Disponible solo si ejecutas Spark Streaming
- Detalles de jobs, stages, tasks
- MÃ©tricas de rendimiento

## ğŸ§ª Testing

```bash
# Probar API
python src/api_client.py

# Probar Dashboard (sin Spark)
python src/dashboard.py

# Con Spark (requiere spark instalado)
spark-submit src/spark_streaming.py
```

## ğŸ“‹ Pasos Siguientes

1. **Entrenamiento de Modelo**
   - Usar datos acumulados en `data/pollution_data.json`
   - Entrenar clasificador de AQI
   - Guardar modelo en `models/`

2. **PredicciÃ³n en Streaming**
   - Cargar modelo entrenado
   - Hacer predicciones en tiempo real
   - Mostrar confianza en dashboard

3. **ComparaciÃ³n de Arquitecturas**
   - Ejecutar en Spark Standalone (local)
   - Ejecutar en Google Colab
   - Ejecutar en AWS
   - Comparar mÃ©tricas de Spark UI

4. **Informe Final**
   - ComparaciÃ³n de desempeÃ±o
   - Screenshots de Spark UI
   - AnÃ¡lisis de resultados
   - Conclusiones

## ğŸ› Troubleshooting

### "ModuleNotFoundError: No module named 'pyspark'"
```bash
pip install pyspark
```

### "ModuleNotFoundError: No module named 'dash'"
```bash
pip install dash plotly
```

### Error de conexiÃ³n a API
- Verifica tu API key en `.env`
- Verifica tu conexiÃ³n a internet
- Comprueba coordenadas (LAT, LON)

### Dashboard no se abre
- AsegÃºrate que el puerto 8050 estÃ¡ disponible
- Prueba: `lsof -i :8050`

## ğŸ“ Notas Importantes

- El update_interval debe ser **mÃ­nimo 300 segundos (5 min)** para no saturar la API gratuita
- Los datos se guardan localmente en `data/pollution_data.json`
- Spark requiere **Java** instalado
- Para GPU support, usa AWS o Google Colab

---

**Proyecto**: ComparaciÃ³n de Arquitecturas de Spark para Streaming de ContaminaciÃ³n del Aire
**Curso**: Arquitectura de Sistemas Distribuidos - ITAM
**Semestre**: OtoÃ±o 2025
